<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Decision Tree]]></title>
    <url>%2F2019%2F04%2F14%2FDecision-Tree%2F</url>
    <content type="text"><![CDATA[About Decision Tree Decision Tree is a kind of common classification and regression algorithm in machine learning, although it's a basic method, some advanced learning algorithms such as GBDT (Gradient Boosting Decision Tree) are established on it. There are approximate three steps in decision tree learning: feature selection, decision tree generating and decision tree pruning. In the essence, decision tree splits the data set according to the information gain, which means this algorithm chooses the features which can make the data set has the minimum uncertainty in each steps. In this blog, I'm going to introduce the feature selection, decision tree generating and decision tree pruning respectively, and the classification and regression tree (CART) will be detailedly interpreted. Feature Selection We need a calculation method or regulation to mark each features in the data set since the essence of decision tree learning is splitting the data set based on features, in other word, we need to determine which feature will be the 'hero' or the 'entrance' in the next level of the decision tree. Therefore, we need to calculate the information gain for the current data set. Entropy and Conditional Entropy For better understanding the information gain, we need to know the concept of entropy first. The entropy is defined as 'a metric can represent the uncertainty of random variable' , assume \(X\) is a random variable, such that the probability distribution is: \[P(X=x_i) = p_i, (i=1,2,3,...,n)\] And the entropy of \(X\) is: \[H(p) = -\sum_{i=1}^n p_i * log(p_i)\] A large entropy represents a large uncertainty, and the range of \(H(p)\) is: \[0 \leq H(p) \leq log(n)\] If we assume there is a random variable \((X,Y)\), and the union probability distribution is: \[P(X=x_i, Y=y_j) = p_{ij}, (i=1,2,...,n; j=1,2,...,m)\] And the conditional entropy is: \[H(Y|X) = \sum_{i=1}^n p_i * H(Y|X=x_i)\] Which represents the uncertainty of random variable \(Y\) with the given random variable \(X\). In this situation, we call entropy and the conditional entropy as empirical entropy and empirical conditional entropy. Information Gain Concept of Information Gain Now we can learn what is information gain. As the definition, 'information gain represents the degree of reduction of the uncertainty of class \(Y\) by giving the feature \(X\)'. Here we use \(D\) to denote the data set, and \(A\) to denote the specific feature, and the information gain of feature \(A\) for data set \(D\) is the difference of empirical entropy and empirical conditional entropy: \[g(D, A) = H(D) - H(D|A)\] We can easily know that different features always have different information gain, a feature with large information has strong ability on classification. Process of Calculating Information Gain calculate the empirical entropy \(H(D)\): \[H(D) = -\sum_{k=1}^K {|C_k|\over |D|} * log_2{|C_k|\over |D|}\] calculate the empirical conditional entropy of feature \(A\): \[H(D|A) =\sum_{i=1}^n {|D_i|\over |D|} * H(D_i)=-\sum_{i=1}^n {|D_i|\over |D|} * \sum_{k=1}^K {|D_{ik}|\over |D_i|} * log_2{|D_{ik}|\over |D_i|}\] calculate the information gain: \[g(D, A) = H(D) - H(D|A)\] Where: \(C_k\) represents the classes, \(|C_k|\) is the amount of class \(C_k\), and \(\sum_{k=1}^K|C_k|=|D|\); \(D_i\) represents the subsets which are splitted according to the feature \(A\), and \(\sum_{i=1}^i|D_i|=|D|\); \(D_{ik}\) represents the subset which all the elements are belong to the class \(C_k\) in the subset \(D_i\) Information Gain Ratio Since splitting the data set by using information gain approach may tends to select the features with more values in some situations, we can use another method named information gain ratio to solve this problem. Information gain ratio, as its name, is the ratio of the information gain and empirical entropy: \[g_R(D,A) = {g(D,A) \over H_A(D)}\] Decision Tree Generation I'm going to introduce two decision tree generating algorithms which are ID3 and C4.5, the former method will be mainly introduced. In addition, the process of generation and pruning of CART will be explained in the next part. ID3 Algorithm ID3 algorithm splits the data set by using the features which have the largest information gain value, and generate the decision tree recursively. The process of the algorithm is: Steps Setting \(T\) as the leaf node and use\(C_k\) as the class label if all the distances in subset \(D\) are belong to class \(C_k\), return \(T\); If there is no further features meet the requirement (the value of information gain less than the threshold), setting \(T\) as the leaf node and use the class of most distances as label, return \(T\); Otherwise, calculating the information gain values of each features for data set \(D\), choosing the largest one as \(A_g\) which will be the splitting point; For each possible values (\(a_i\)) in \(A_g\), splitting the data set into plenty of subsets \(D_i\) by \(A_g = a_i\), use the class of most distances in each subset \(D_i\) as labels, return \(T\); For each sub-nodes in step 4, use \(D_i\) as training set, \(A-A_g\) as feature set, execute the step 1 to 4 recursively until meet the stopping conditions, return \(T_i\) Code Import the necessary third-party libraries : 1234567891011121314import numpy as npimport pandas as pdimport matplotlib.pyplot as plt%matplotlib inlinefrom sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom collections import Counterimport mathfrom math import logimport sysimport pprint First we calculate the information gain value : 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# empirical entropydef cal_entropy(self, datasets): n = len(datasets) label_count = &#123;&#125; # get distribution(Pi) for i in range(n): label = datasets[i][-1] if label not in label_count: label_count[label] = 0 label_count[label] += 1 empirical_entropy = -sum([(p/n) * log(p/n, 2) for p in label_count.values()]) return empirical_entropy# empirical conditional entropydef cal_conditional_entropy(self, datasets, axis=0): n = len(datasets) feature_sets = &#123;&#125; for i in range(n): feature = datasets[i][axis] if feature not in feature_sets: feature_sets[feature] = [] feature_sets[feature].append(datasets[i]) empirical_conditional_entropy = sum([(len(p)/n) * self.cal_entropy(p) for p in feature_sets.values()]) return empirical_conditional_entropy# calculate the difference between two entropydef info_gain(self, entropy, con_entropy): return entropy - con_entropy# calculate information gaindef get_info_gain(self, datasets): feature_count = len(datasets[0]) - 1 empirical_entropy = self.cal_entropy(datasets) best_feature = [] for c in range(feature_count): c_info_gain = self.info_gain(empirical_entropy, self.cal_conditional_entropy(datasets, axis=c)) best_feature.append((c, c_info_gain)) best = max(best_feature, key=lambda x : x[-1]) # best : ((feature_id, feature_info_gain)) return best Define the class Node as : 123456789101112class Node: def __init__(self, splitting_feature_id=None, class_label=None, data=None, splitting_feature_value=None): self.splitting_feature_id = splitting_feature_id # splitting feature self.splitting_feature_value = splitting_feature_value # splitting feature value self.class_label = class_label # class label, only leaf has self.data = data # labels of samples, only leaf has self.child = [] # child node def add_node(self, node): self.child.append(node) Then achieve the ID3 algorithm : 123456789101112131415161718192021222324252627282930313233343536373839404142def train(self, train_data, node): _ = train_data.iloc[:, :-1] y_train = train_data.iloc[:, -1] features = train_data.columns[:-1] # 1. if all the data in D belong to the same class C, # set T as single node and use C as the label, return T if len(y_train.value_counts()) == 1: node.class_label = y_train.iloc[0] node.data = y_train return # 2. if feature A is empty, set T as single node and use the label, # most C as the label, return T if len(features) == 0: node.class_label = y_train.value_counts().sort_values(ascending=False).index[0] node.data = y_train return # 3. calculate the largest inforamtion gain, use Ag to represents the best feature max_feature_id, max_info_gain = self.get_info_gain(np.array(train_data)) max_feature_name = features[max_feature_id] # 4. if the information gain is smaller than threshold, set T as single node, # and use the most C as the label, return T if max_info_gain &lt;= self.epsilon: node.class_label = y_train.value_counts().sort_values(ascending=False).index[0] node.data = y_train return # 5. splitting D according to each possible values in the feature A feature_list = train_data[max_feature_name].value_counts().index for Di in feature_list: node.splitting_feature_id = max_feature_id child = Node(splitting_feature_value = Di) node.add_node(child) sub_train_data = pd.DataFrame([list(i) for i in train_data.values if i[max_feature_id] == Di], columns = train_data.columns) # 6. create tree recursively self.train(sub_train_data, child) C4.5 Algorithm The C4.5 algorithm becomes extremely easy to understand after introducing the ID3 algorithm, cause there's only one different point between the two methods, that is C4.5 algorithm uses information gain ratio to choose the splitting feature instead of using information gain, besides this, other steps in C4.5 are same with those in ID3 algorithm. Decision Tree Pruning A Simple Pruning approach In this part, we just discuss the pruning algorithm for ID3 and C4.5, the pruning for CART will be interpreted in the next part. We have discussed the decision tree generating approached in the above part, however, it can overfit easily if there're lots of levels in the decision tree, so we need to prune the tree to avoid overfitting and simplify the process of calculating. Specifically, we cut some leaf nodes or sub-tree from the original tree and use their parents nodes as the new leaf nodes. Like the other machine learning algorithms, here we prune the decision tree by minimising the loss function. Assume that \(|T|\) is the number of leaf nodes, \(t\) represents the leaf nodes and there are \(N_t\) samples in it, use \(N_{tk}\) to denote the number of samples which belong to class \(k\) in \(N_t\), where \(k=1,2,...,K\) ; \(H_t(T)\) is the empirical entropy of leaf node \(t\), then define the loss function as : \[C_\alpha(T) = \sum_{t=1}^{|T|}N_t*H_t(T) + \alpha|T|\] We define the training error as : \[C(T) = \sum_{t=1}^{|T|}N_t*H_t(T) = -\sum_{t=1}^{|T|}\sum_{k=1}^KN_{tk} * log {N_{tk}\over N_t}\] In the above formula, the term \(|T|\) which the number of leaf nodes, can represent the complexity of decision tree, the parameter \(\alpha\), can be understood as the coefficient of regularization, so the term \(\alpha|T|\) actually has the same function with regularization term, which can find the trade-off between the complexity and precision of model. A larger \(\alpha\) can promote to choose a simple model and a smaller \(\alpha\) promote to choose a complex model. Our pruning algorithm is going to find the sub-tree with the minimum loss function, here we calculate the loss value \(C_\alpha(T)\) for the current tree, then calculating it again after cutting the brunch (sub-tree) or leaf node, now we get two different information gain values, record as \(C_\alpha(T_B)\) and \(C_\alpha(T_A)\), so we can determine whether it's worth to cut this brunch or not by comparing the two error values, the specific steps are : Calculating the empirical entropy for each leaf nodes; Going back to the parents nodes recursively, cutting the brunch and comparing the error values between before cutting and after cutting, if \(C_\alpha(T_A) \leq C_\alpha(T_B)\), saving the pruning action and setting the parent node as the new leaf node, otherwise, recover the original tree; Repeating the step 2 until all the nodes have been checked, then get the sub-tree with the minimum loss function Code First, we need to achieve the loss function which is the \(C_\alpha(T)\) in the above formula : 1234567891011121314151617181920212223242526272829303132# calculate C_alpha_T for current sub-treedef c_error(self): leaf = [] self.find_leaf(self.tree, leaf) # count the N_t, len(leaf_num) == |T| leaf_num = [len(l) for l in leaf] # calculate empirical entropy for each leaf nodes entropy = [self.cal_entropy(l) for l in leaf] # alpha * |T| alpha_T = self.alpha * len(leaf_num) error = 0 C_alpha_T = 0 + alpha_T for Nt, Ht in zip(leaf_num, entropy): C_T = Nt * Ht error += C_T C_alpha_T += error return C_alpha_T # find all leaf nodes def find_leaf(self, node, leaf): for t in node.child: if t.class_label is not None: leaf.append(t.data) else: for c in node.child: self.find_leaf(c, leaf) Then calculate loss value for the original tree, starting pruning : 1234567def pruning(self, alpha=0): if alpha: self.alpha = alpha error_min = self.c_error() self.find_parent(self.tree, error_min) In which the find_parent function corresponds the step 2 in the above principle introduction part : 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970def find_parent(self, node, error_min): ''' leaf nodes : class_label -&gt; not None data -&gt; not None splitting_feature_id -&gt; None splitting_feature_value -&gt; None child -&gt; None --------------------------------------------- other nodes: class_label -&gt; None data -&gt; None splitting_feature_id -&gt; not None splitting_feature_value -&gt; not None(except root) child -&gt; not None ''' # if not the leaf nodes if node.splitting_feature_id is not None: # collect class_labels from child nodes class_label = [c.class_label for c in node.child] # if all the child nodes are leaf nodes if None not in class_label: # collect data from child nodes child_data = [] for c in node.child: for d in list(c.data): child_data.append(d) child_counter = Counter(child_data) # copy the old node old_child = node.child old_splitting_feature_id = node.splitting_feature_id old_class_label = node.class_label old_data = node.data # pruning node.splitting_feature_id = None node.class_label = child_counter.most_common(1)[0][0] node.data = child_data error_after_pruning = self.c_error() # if error_after_pruning &lt;= error_min, it is worth to pruning if error_after_pruning &lt;= error_min: error_min = error_after_pruning return 1 # if not, recover the previous tree else: node.child = old_child node.splitting_feature_id = old_splitting_feature_id node.class_label = old_class_label node.data = old_data # if not all the child nodes are leaf nodes else: re = 0 i = 0 while i &lt; len(node.child): # if the pruning action happend, # rescan the sub-tree since some new leaf nodes are created if_re = self.find_parent(node.child[i], error_min) if if_re == 1: re = 1 elif if_re == 2: i -= 1 i += 1 if re: return 2 return 0 CART As the name of this algorithm -- classification and regression tree, it can apply on both classification and regression, actually the essence of regression tree is based on classification principle, I'm going to explain it later. Regression Tree Principle of Generating The process of generating regression tree is also the procedure of establishing binary decision tree recursively by minimising the mean square error. In the regression problems, we assume the output \(Y\) is continuous variable, so the training set is : \[D = \{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}\] Assume that we have splitted the data set into \(M\) units: \(R_1, R2,...,R_M\) , and there are one unique output value in each units \(R_m\), record as \(c_m\), in other word, we split the training set into plenty of units, and calculate an output or prediction value for each subsets, therefore, the model can be represented as : \[f(x) = \sum_{m=1}^Mc_m\] And we use the mean square error to evaluate the training error for our model : \[\sum(y_i-f(x_i))^2\] In addition, the optimal output value of \(R_m\) is : \[c_m = average(y_i|x_i \in R_m)\] Now, we need to choose the appropriate feature and value to split the data set, we call the chosen feature as splitting variable and the value is splitting point References]]></content>
  </entry>
  <entry>
    <title><![CDATA[Recommeder System Reading Notes]]></title>
    <url>%2F2019%2F04%2F05%2FRecommeder-System-Reading-Notes%2F</url>
    <content type="text"><![CDATA[Recommender System Reading Notes Chapter One (Introduction) Basic Concepts Collaborative Recommendation Recommend something to users according to the behavior of other users who have the same interests or similar historical behaviors with them, this technique is called collaborative filtering. Content-Based Recommendation Recommend things depend on the content which has two advantages: Doesn't require large user groups New items can be immediately recommended once item attributes are available. Knowledge-Based Recommendation Some cases may involve larget numbers of one-time buyers which means can't reyly the purchase history. We can use knowledge-based recommendation such as constraint-based approach, we recommend rely on the feature of product so that every user will get the same set of recommendations. In addition, since we don't have any purchase history, we can just ask users for their requirements directly, such as the maximum price and so on. Hybrid Approaches Since almost all methods have their own disadvantages, we can try to combine them to generate better or more precise recommendations. Chapter Two (Collaborative Recommendation) User-based nearest neighbor recommendation Given a ratings database and the ID of the current (active) user as an input, identify other users (sometimes referred to as peer users or nearest neighbors) that had similar preferences to those of the active user in the past. ### Similarity between users We can use Pearson correlation coefficient to calculate the similarity between two different users： \[\rho_{x,y} = {cov(x,y)\over{\sigma{x}\sigma{y}}} = {E((x-\mu{x})(y-\mu{y}))\over{\sigma{x}\sigma{y}}}\] And \(cov\) is Covariance, \(E\) is mathmatical expectation, \(\sigma\) is standard deviation. We denote \(U = {u_1,...,u_n}\) as the set of users, use \(P = {p_1,...,p_m}\) to denote products, and \(R\) as an \(n*m\) matrix of ratings \(r_{i,j}\), so: And then we can use a possible formula to make prediction: But, this measure method does not take into account whether two users have co-rated only a few items (on which they may agree by chance) or whether there are many items on which they agree. In fact, it has been shown that predictions based on the ratings of neighbors with which the active user has rated only a very few items in common are a bad choice and lead to poor predictions. Neighborhood Selection The common techniques for reducing the size of the neighborhood are to define a specific minimum threshold of user similarity or to limit the size to a fixed number and to take only the \(k\) nearest neighbors into account. Item-based nearest neighbor recommendation Item-based recommendation, which is more apt for offline preprocessing and thus allows for the computation of recommendations in real time even for a very large rating matrix. The main idea of item-based algorithms is to compute predictions using the similarity between items and not the similarity between users. The cosine similarity measure The similarity between two items a and b – viewed as the corresponding rating vectors a⃗ and b⃗ – is formally defined as follows: The basic cosine measure does not take the differences in the average rating behavior of the users into account. This problem is solved by using the adjusted cosine measure, which subtracts the user average from the ratings. The adjusted cosine measure is then calculated as follows: Formally, we can predict the rating for user u for a product p as follows: Preprocessing data for item-based filtering It is an option to exploit only a certain fraction of the rating matrix to reduce the computational complexity. Basic techniques include subsampling, which can be accomplished by randomly choosing a subset of the data or by ignoring customer records that have only a very small set of ratings or that only contain very popular items. About ratings Implicit and Explicit ratings It's not easy to get the explicit ratings from users in some cases. Data Sparsity In real-world applications, of course, the rating matrices tend to be very sparse, as customers typically provide ratings for (or have bought) only a small fraction of the catalog items. One straightforward option for dealing with this problem is to exploit additional information about the users, such as gender, age, education, interests, or other available information that can help to classify the user. The Cold-Start Problems The main idea of the approach to solve the cold-start problem is to exploit the supposed “transitivity” of customer tastes and thereby augment the matrix with additional information (external information).]]></content>
      <categories>
        <category>Recommender System</category>
      </categories>
      <tags>
        <tag>Recommender System</tag>
        <tag>Reading Notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Regularization in Machine Learnig]]></title>
    <url>%2F2019%2F04%2F03%2FRegularization-in-Machine-Learnig%2F</url>
    <content type="text"><![CDATA[Regularization in Machine Learning About Regularization The model will become overfitting when it is trying too hard to capture the outliers in the training dataset and the accuracy will have a low accuarcy in the testing set. Regularization is one of the most efficient methods for avoiding overfitting, in the essence, this approach could penalise weights(coefficients) in the model by making some of them toward to zero. What is the problem? Assume that we have an empirical loss function R: \[R_{emp} = {1\over{N}}\sum_{i=1}^NL(f(x_i), y_i)\] and the model function f: \[f(x_i) = w_0x_0 + w_1x_1 + w_2x_2 + w_3x_3 + ... + w_nx_n\] We can see the polynomial f is so complicated if there is no zero or very less zero in metrix w (the set of weights), and the model is also complex, which means this model can just fit well in the training set, in other word, it has a poor generalization ability. Obviously, we do not need that many non-zero and too large weights since not every features is useful in the estimating. Regulariztion is the approach that can cope this trouble. Regularization function can be understood as the complxity of model (cause it is a function about weights), it can be the norm of the model parameter vector, and different choices have different constrains on the weights, so the effect is also different. Actually, both a large amount of weights and weights with large values can lead to a complex model. The derivative will have a large fluctuation if our model is trying to fit the outlier in a specific interval, and a large derivative means the weight will also have a large value, therefore, a complex model always have parameters with large values. In the essence of solving overfitting problem, reularization term reduces the complexity of model from controlling the number and the value of parameters, the term is a constraint for the parameters when we are trying to minimize the loss function. L-P Norm We can understand norm as distance, it represents the size of vectors in vector space and the size of changing in matrix space. L-P norm is a set of norms: \[Lp = \sqrt[p]{\sum_{i=1}^Nx_i^p}\] Norms change accroding to the values of p: The above picture shows how the changes of the points whose distance (norm) to the origin is 1 in the three dimensional space. L0 Norm Normally we use L0 norm to represent the number of non-zero elements in a vector. Therefore, minimize L0 norm meams we want some weights become to zero, so that some features will not use in estimating, we can deduce the complexity of model through using L0 norm, unfortunately, it is hard to explain and calculate L0 norm since we can not interpret the meaning of zero-sqrt, and optimization of L0 norm is a NP hard question, but do not worry, we can use L1 norm to subtitue, which is an optimal convex approximation of L0 norm. L1 Norm This is the defination of L1 norm: \[||x||_1 = \sum_i{|x_i|}\] It represents the sum of the absolute values of non-zero elements in vector x. Due to the natural nature of L1 norm, the solution to L1 norm optimization is a sparse solution, we can use L1 norm to achieve the sparse of features, so that some unuseful features will be dropped. L2 Norm The defination of L2 norm is: \[||x||_2 = \sqrt{\sum_ix_i^2}\] We can make the value of weights toward to zero by minimizing L2 norm, but it is just toward instead of reaching at zero, which is different from L0 norm and L1 norm. Although there are several differences among L0, L1 and L2 norm, the essence of them is same, we use them to weaken or even drop some features, so that we can get a simpler model with a better generization ability. How it works? It is easy to answer this question after konwing the concept of norm: we just need to optimize the L1 or L2 norm, it can penalise weights atuomaticlly. However, do not forget our true aim: fitting data by minimizing the loss function. Therefore, we need to minimize loss function and regularization term simultaneously, but it is still easy to do that, we can minimize the sum of them: \[min{[R_{emp} + \lambda{r(w)}]}\] That is: \[min[{1\over{N}}\sum_{i=1}^NL(f(x_i), y_i) + {\lambda\over{2}}{||w||_2^2}]\] A larger lambda can impel to chose a simpler model and a smaller lambda will get a more complex model. Why L1 is sparse and L2 is smooth? As we said, L1 norm can make weights change to zero and L2 norm let the parameters toward to zero, we call L1 norm is sparse and L2 norm is smooth. We can understand this from different aspects. Mathmatical Formula Firstly, we assume the structural loss function with L1 norm is: \[L1 = R + {\lambda\over{n}}\sum_i|w_i|\] So we can calculate the derivative of w: \[{\partial{L1}\over{\partial{w}}}={\partial{R}\over{\partial{w}}}+{\lambda\over{n}}sign(w)\] And then: \[w^{t+1} = w^t - \eta{\partial{L1}\over{\partial{w^t}}}\] \[w^{t+1}=w^t-\eta{\partial{R}\over{\partial{w^t}}}-\eta{\lambda\over{n}}sign(w^t)\] And sign function is defined as: \[sign(x) = \begin{cases} +1 &amp; x&gt;0 \\ -1 &amp; x&lt;0 \\ [-1,1] &amp; x=0 \end{cases}\] As for the structural risk function with L2 norm: \[L2 = R + {\lambda\over{2n}}\sum_iw_i^2\] And then: \[{\partial{L2}\over{\partial{w}}}={\partial{R}\over{\partial{w}}}+{\lambda\over{n}}w\] \[w^{t+1} = w^t - \eta{\partial{L2}\over{\partial{w^t}}}\] \[w^{t+1}=w^t-\eta{\partial{R}\over{\partial{w^t}}}-\eta{\lambda\over{n}}w^t\] \[w^{t+1}=(1-\eta{\lambda\over{n}})w^t-\eta{\partial{R}\over{\partial{w^t}}}\] So we can see that the difference between the two formulas: the parameter w in \(L1\) minus \(\eta{\lambda\over{n}}sign(w)\) which is a constant for each \(w\) (see the deifnation of \(sign(x)\)). However, in \(L2\) it minus \(\eta{\lambda\over{n}}w\), which means \(w\) will reduce in a specific proportion in each iterations, and the proportion is \(\eta{\lambda\over{n}}\). Therefore, in some extent, L1 norm can make the parameteres(\(w\)) reduce to zero since it minus a constant each time, and L2 norm makes the weights toward to zero because reducing with a specific proportion. This is also the reason why we can use L1 norm to do feature selection(automaticlly) and use L2 norm to make the parameters smaller, and we also call L2 norm as weight decay. In addition, L2 norm has a more quicker reduction rate than L1 norm when \(w\) is in the interval \([1, +\infty]\), and L1 norm has a faster decreasing rate when \(w\) is in \((0, 1)\). So L1 norm can make \(w\) reach at zero more easier and get more zero elements in the parameter vector. Geometric Space We can also understand this question from geometric space. For the above picture, we assume that the red line is our empirical risk function(R), the light blue circle is the graph of L2 norm and the deep blue one is L1 norm. As we said, L1 and L2 norms are a kind of constraint for the function R, therefore, the intersection is our best solution, we can see that L1 norm can more easier to make the solution reach at zero. The same situation can be found in other empirical loss function, such as squared error: I am going to introduce another interpretation in geometric space which I believe can help us to understand this question better. As the above graph shows, assume that the purple line is our empirical loss function (R), and the green point is the local minimum. Now, we add L2 norm to the function, so we can get \(R+\lambda x^2\): The minimun has changed to the yellow point, it can be seen that the optimum value has reduced, but is not zero. We can get the new function \(R+\lambda |x|\) with L1 norm: The optimum changed to zero (the pink line). Remember that the truth is L1 norm can get optimum solution as zero more easier than L2 norm, which means we can not make sure the best value must be zero with L1 norm and L2 norm can also get zero value solution in the specific situation. I am going to interpret this situation. Actually, whether the two norms can change the optimal solution (the parameter/weight) to zero is depend on the value of derivative of zero point (x=0) in the empirical loss function: the derivative of zero point can not change to zero so the optimal solution is also non-zero if the original function (R) has non-zero derivative in the point x=0, in other words, L2 norm can change the optimal value to zero if the derivative of \(R_{emp}\) is zero in x=0. In terms of L1 norm, x=0 can be the optimal solution (local minimum) if the regularization coefficient \(\lambda\) is larger than the value of \(|R_{emp}&#39;(0)|\) (since we need to guarantee the derivates of left side and right side of x=0 have the opposite sign): The derivative of left side: \[R_{emp}&#39;(0)-\lambda\] The derivative of right side: \[R_{emp}&#39;(0)+\lambda\] Therefore, if: \[\lambda \geq |R_{emp}&#39;(0)|\] Then we can guarantee x=0 is the local minimum. Probability Degree Some Papers Reference l1 相比于 l2 为什么容易获得稀疏解？ 机器学习算法系列（28）：L1、L2正则化 聊一聊L1和2正则化 L1,L2,L0区别，为什么可以防止过拟合 机器学习中的范数规则化之（一）L0、L1与L2范数 常用的范数求导 统计学习方法]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Regularization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROC and AUC]]></title>
    <url>%2F2019%2F04%2F01%2FROC-and-AUC%2F</url>
    <content type="text"><![CDATA[ROC &amp; AUC Confusion Matrix I am going to introduce the concept of Confusion Matrix before talking about the ROC as well as AUC since I think this will help us to understand them better. Confusion Matrix is a performance measurement for machine learning classification, which is a specific table layout. Let us undetstand TP, FP, FN and TN first. As the above picture show, we define them as: TP(True Positive): The samples which are positive and have been predicted as positive FP(False Positive): The samples which are negative but have been predicted as positive FN(False Negative): The samples which are positive but have been predicted as negative TN(True Nagetive): The samples which are negative and have been predicted as negative So we can calculate the Recall and Precision easily. Recall means out of all the positive classes, how much we predicted correctly: \[Recall = {TP\over{TP + FN}}\] Precision means out of all the classes, how much we predicted correctly: \[Precision = {TP\over{TP + FP}}\] And we can also use F-Score (F1 or F-Measure), which can measure recall and precision simultaneously. F-Score is the harmonic mean of recall and precision: \[F_1 = ({Recall^{-1}+Precision^{-1}\over{2}})^{-1} ={2*Recall*Precision\over{Recall + Precision}}\] \[F_\beta = (1+\beta^2)*{Precision*Recall\over{(\beta^2*Precision)+Recall}}\] Where β is the weight of F-Score, it will give more proportion to Precision when β &lt; 1 and Recall gets a higher proportion when β &gt; 1, β = 1 is a specific situation which Recall and Precision has the same weight. However, it is difficult to compare two models with low precision and high recall or high precision and low recall since their F-Score could be similar. Here we have another two names: Sensitivity and Specificity: \[Sensitivity = Recall = True Positive Rate (TPR)\] \[Specificity = 1 - False Positive Rate (FPR) = 1 - {FP\over{FP + TN}}\] We can try to understand these from conditional probability aspect. Assume that our predict vales is Y' and Y is the true vale, so that: \[Precision = P(Y=1|Y&#39;=1)\] \[Recall = Sensitivity = P(Y&#39;=1|Y=1)\] \[Specificity = P(Y&#39;=0|Y=0)\] It can be seen that Recall (Sensitivity) and Specificity do not be influenced by imbalanced data but Precision relies on the proportion of positive and negative samples. The Precision-Recall Curve What's the problem of accuracy? Accuracy is one of the most common evaluation methods in machine learning, but it is not always appropriate: assume we have 100 samples in the test set, and there are 99 negative samples and just 1 positive sample, so that the accuracy of my model is 99/100 = 99% if it predicts all samples as negative class. Obviously, it can not evaluate our model in this case. \[Accuracy = {TP + TN\over{TP+TN+FP+FN}}\] What is ROC? ROC(Receiver Operating Characteristics) curve is a kind of tool which we can use to check or visualize the performance of the multi-classification models. ROC is based on confusion matrix which use FPR as x-axis and TPR as y-axis. We go through all the thresholds and draw the ROC graph, so that each points in ROC represents the FPR and TRP value in a specific threshold. How to evaluate ROC? The ROC will not change even we modify the threshold, the x-axis, FPR value can be understand as how much we predicted negative samples as positive which should be lower, and the y-axis, TPR value is how much we predicted correctly in the positive samples which should be higher, therefore, the performance of a model is good with high TPR and low FPR, which reflects as a steep ROC curve (The curve should tend to upper left corner). In addition, as we said Recall (Sensitivity, TPR) and Specificity (1-FPR) do not be influenced by imbalanced, so ROC curve will also not influenced by imbalanced data: What is AUC? AUC (Area Under the Curve) is the area under the ROC curve, the diagonal represents random effect which value is 0.5, positive and negative samples occupy the same proportion is the data set in this specific situation. The best value of AUC is 1 but can not reach in the most situation, and the worst is 0.5 which means classify samples ramdomly. For example, AUC is 0.7 means there is 70% chance that model will be able to distinguish between positive class and negative class. Relation between TPR and FPR As we know FPR is 1 - specificity, so when we increase TPR, FPR also increases and vice versa. Achievement by Python 12345678910111213141516from sklearn import metricsfrom sklearn.metics import f1_scoreimport numpy as np# True lablesy_true = np.array([1, 1, 2, 2]) # Probability estimates of the positive classy_scores = np.array([0,1, 0.4, 0.35, 0.8])auc_score = metrics.roc_auc_score(y_true, y_scores)&gt;&gt;&gt;print(auc_score)0.75y_true = [0, 1, 2, 0, 1, 2]y_pred = [0, 2, 1, 0, 0, 1]&gt;&gt;&gt;print(f1_score(f1_score(y_true, y_pred, average=&apos;micro&apos;)))0.33... Reference Confusion matrix Wikipedia 精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么？ Understanding AUC - ROC Curve]]></content>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Model Evaluation</tag>
      </tags>
  </entry>
</search>
