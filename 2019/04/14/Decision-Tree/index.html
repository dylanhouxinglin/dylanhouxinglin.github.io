<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic|Lora:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|Consolas:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Machine Learning,Decision Tree,">










<meta name="description" content="About Decision Tree Decision Tree is a kind of common classification and regression algorithm in machine learning, although it&apos;s a basic method, some advanced learning algorithms such as GBDT (Gradien">
<meta name="keywords" content="Machine Learning,Decision Tree">
<meta property="og:type" content="article">
<meta property="og:title" content="Decision Tree">
<meta property="og:url" content="http://yoursite.com/2019/04/14/Decision-Tree/index.html">
<meta property="og:site_name" content="Just For Fun">
<meta property="og:description" content="About Decision Tree Decision Tree is a kind of common classification and regression algorithm in machine learning, although it&apos;s a basic method, some advanced learning algorithms such as GBDT (Gradien">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/images/decision_tree/decision_tree_1.png">
<meta property="og:updated_time" content="2019-04-25T07:33:36.683Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Decision Tree">
<meta name="twitter:description" content="About Decision Tree Decision Tree is a kind of common classification and regression algorithm in machine learning, although it&apos;s a basic method, some advanced learning algorithms such as GBDT (Gradien">
<meta name="twitter:image" content="http://yoursite.com/images/decision_tree/decision_tree_1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"right","display":"always","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/04/14/Decision-Tree/">





  <title>Decision Tree | Just For Fun</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Just For Fun</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/14/Decision-Tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="dylan_houxinglin">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Just For Fun">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Decision Tree</h1>
        

        <div class="post-meta">
	
    <font color="#FFFFFF"><b style="background-color:#A9A9A9">　Top　</b></font>
    	<span class="post-meta-divider">|</span>
		
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-14T23:50:17+08:00">
                2019-04-14
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2019-04-25T15:33:36+08:00">
                2019-04-25
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  5.8k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  36
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="about-decision-tree">About Decision Tree</h1>
<p>Decision Tree is a kind of common classification and regression algorithm in machine learning, although it's a basic method, some advanced learning algorithms such as GBDT (Gradient Boosting Decision Tree) are established on it. <a id="more"></a>There are approximate three steps in decision tree learning: feature selection, decision tree generating and decision tree pruning. In the essence, decision tree splits the data set according to the information gain, which means this algorithm chooses the features which can make the data set has the minimum uncertainty in each steps. In this blog, I'm going to introduce the feature selection, decision tree generating and decision tree pruning respectively, and the classification and regression tree (CART) will be detailedly interpreted.</p>
<p><strong>All the codes in this atricle are collected on my github: <a href="https://github.com/dylanhouxinglin/lihang-statistical-learning-method/tree/master/lihang-code/DecisionTree" target="_blank" rel="noopener">Decision Tree</a></strong></p>
<h1 id="feature-selection">Feature Selection</h1>
<p>We need a calculation method or regulation to mark each features in the data set since the essence of decision tree learning is splitting the data set based on features, in other word, we need to determine which feature will be the 'hero' or the 'entrance' in the next level of the decision tree. Therefore, we need to calculate the <strong>information gain</strong> for the current data set.</p>
<h2 id="entropy-and-conditional-entropy">Entropy and Conditional Entropy</h2>
<p>For better understanding the information gain, we need to know the concept of <strong>entropy</strong> first. The entropy is defined as <strong>'a metric can represent the uncertainty of random variable'</strong> , assume <span class="math inline">\(X\)</span> is a random variable, such that the probability distribution is: <span class="math display">\[P(X=x_i) = p_i, (i=1,2,3,...,n)\]</span> And the entropy of <span class="math inline">\(X\)</span> is: <span class="math display">\[H(p) = -\sum_{i=1}^n p_i * log(p_i)\]</span> <strong>A large entropy represents a large uncertainty</strong>, and the range of <span class="math inline">\(H(p)\)</span> is: <span class="math display">\[0 \leq H(p) \leq log(n)\]</span> If we assume there is a random variable <span class="math inline">\((X,Y)\)</span>, and the union probability distribution is: <span class="math display">\[P(X=x_i, Y=y_j) = p_{ij}, (i=1,2,...,n; j=1,2,...,m)\]</span> And the conditional entropy is: <span class="math display">\[H(Y|X) = \sum_{i=1}^n p_i * H(Y|X=x_i)\]</span> Which represents the uncertainty of random variable <span class="math inline">\(Y\)</span> with the given random variable <span class="math inline">\(X\)</span>. In this situation, we call entropy and the conditional entropy as empirical entropy and empirical conditional entropy.</p>
<h2 id="information-gain">Information Gain</h2>
<h3 id="concept-of-information-gain">Concept of Information Gain</h3>
<p>Now we can learn what is <strong>information gain</strong>. As the definition, <strong>'information gain represents the degree of reduction of the uncertainty of class <span class="math inline">\(Y\)</span> by giving the feature <span class="math inline">\(X\)</span>'</strong>. Here we use <span class="math inline">\(D\)</span> to denote the data set, and <span class="math inline">\(A\)</span> to denote the specific feature, and the information gain of feature <span class="math inline">\(A\)</span> for data set <span class="math inline">\(D\)</span> is the difference of empirical entropy and empirical conditional entropy: <span class="math display">\[g(D, A) = H(D) - H(D|A)\]</span> We can easily know that <strong>different features always have different information gain, a feature with large information has strong ability on classification</strong>.</p>
<h3 id="process-of-calculating-information-gain">Process of Calculating Information Gain</h3>
<ol type="1">
<li><p>calculate the empirical entropy <span class="math inline">\(H(D)\)</span>: <span class="math display">\[H(D) = -\sum_{k=1}^K {|C_k|\over |D|} * log_2{|C_k|\over |D|}\]</span></p></li>
<li><p>calculate the empirical conditional entropy of feature <span class="math inline">\(A\)</span>: <span class="math display">\[H(D|A) =\sum_{i=1}^n {|D_i|\over |D|} * H(D_i)=-\sum_{i=1}^n  {|D_i|\over |D|} * \sum_{k=1}^K {|D_{ik}|\over |D_i|} * log_2{|D_{ik}|\over |D_i|}\]</span></p></li>
<li><p>calculate the information gain: <span class="math display">\[g(D, A) = H(D) - H(D|A)\]</span> Where:</p></li>
</ol>
<ul>
<li><p><span class="math inline">\(C_k\)</span> represents the classes, <span class="math inline">\(|C_k|\)</span> is the amount of class <span class="math inline">\(C_k\)</span>, and <span class="math inline">\(\sum_{k=1}^K|C_k|=|D|\)</span>;</p></li>
<li><p><span class="math inline">\(D_i\)</span> represents the subsets which are splitted according to the feature <span class="math inline">\(A\)</span>, and <span class="math inline">\(\sum_{i=1}^i|D_i|=|D|\)</span>;</p></li>
<li><p><span class="math inline">\(D_{ik}\)</span> represents the subset which all the elements are belong to the class <span class="math inline">\(C_k\)</span> in the subset <span class="math inline">\(D_i\)</span></p></li>
</ul>
<h3 id="information-gain-ratio">Information Gain Ratio</h3>
<p>Since splitting the data set by using information gain approach may tends to select the features with more values in some situations, we can use another method named <strong>information gain ratio</strong> to solve this problem. Information gain ratio, as its name, is the ratio of the information gain and empirical entropy: <span class="math display">\[g_R(D,A) = {g(D,A) \over H_A(D)}\]</span></p>
<h1 id="decision-tree-generation">Decision Tree Generation</h1>
<p>I'm going to introduce two decision tree generating algorithms which are ID3 and C4.5, the former method will be mainly introduced. In addition, the process of generation and pruning of CART will be explained in the next part.</p>
<h2 id="id3-algorithm">ID3 Algorithm</h2>
<p>ID3 algorithm splits the data set by using the features which have the largest information gain value, and generate the decision tree recursively. The process of the algorithm is:</p>
<h3 id="steps">Steps</h3>
<ol type="1">
<li><p>Setting <span class="math inline">\(T\)</span> as the leaf node and use<span class="math inline">\(C_k\)</span> as the class label if all the distances in subset <span class="math inline">\(D\)</span> are belong to class <span class="math inline">\(C_k\)</span>, return <span class="math inline">\(T\)</span>;</p></li>
<li><p>If there is no further features meet the requirement (the value of information gain less than the threshold), setting <span class="math inline">\(T\)</span> as the leaf node and use the class of most distances as label, return <span class="math inline">\(T\)</span>;</p></li>
<li><p>Otherwise, calculating the information gain values of each features for data set <span class="math inline">\(D\)</span>, choosing the largest one as <span class="math inline">\(A_g\)</span> which will be the splitting point;</p></li>
<li><p>For each possible values (<span class="math inline">\(a_i\)</span>) in <span class="math inline">\(A_g\)</span>, splitting the data set into plenty of subsets <span class="math inline">\(D_i\)</span> by <span class="math inline">\(A_g = a_i\)</span>, use the class of most distances in each subset <span class="math inline">\(D_i\)</span> as labels, return <span class="math inline">\(T\)</span>;</p></li>
<li><p>For each sub-nodes in step 4, use <span class="math inline">\(D_i\)</span> as training set, <span class="math inline">\(A-A_g\)</span> as feature set, execute the step 1 to 4 recursively until meet the stopping conditions, return <span class="math inline">\(T_i\)</span></p></li>
</ol>
<h3 id="code">Code</h3>
<p>Import the necessary third-party libraries : <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> pprint</span><br></pre></td></tr></table></figure></p>
<p>First we calculate the information gain value : <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># empirical entropy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_entropy</span><span class="params">(self, datasets)</span>:</span></span><br><span class="line">    n = len(datasets)</span><br><span class="line">    label_count = &#123;&#125;</span><br><span class="line">    <span class="comment"># get distribution(Pi)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        label = datasets[i][<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">not</span> <span class="keyword">in</span> label_count:</span><br><span class="line">            label_count[label] = <span class="number">0</span></span><br><span class="line">        label_count[label] += <span class="number">1</span></span><br><span class="line">    empirical_entropy = -sum([(p/n) * log(p/n, <span class="number">2</span>) <span class="keyword">for</span> p <span class="keyword">in</span> label_count.values()])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> empirical_entropy</span><br><span class="line"></span><br><span class="line"><span class="comment"># empirical conditional entropy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_conditional_entropy</span><span class="params">(self, datasets, axis=<span class="number">0</span>)</span>:</span></span><br><span class="line">    n = len(datasets)</span><br><span class="line">    feature_sets = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        feature = datasets[i][axis]</span><br><span class="line">        <span class="keyword">if</span> feature <span class="keyword">not</span> <span class="keyword">in</span> feature_sets:</span><br><span class="line">            feature_sets[feature] = []</span><br><span class="line">        feature_sets[feature].append(datasets[i])</span><br><span class="line"></span><br><span class="line">    empirical_conditional_entropy = sum([(len(p)/n) * self.cal_entropy(p)</span><br><span class="line">                                        <span class="keyword">for</span> p <span class="keyword">in</span> feature_sets.values()])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> empirical_conditional_entropy</span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate the difference between two entropy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">info_gain</span><span class="params">(self, entropy, con_entropy)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> entropy - con_entropy</span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate information gain</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_info_gain</span><span class="params">(self, datasets)</span>:</span></span><br><span class="line">    feature_count = len(datasets[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">    empirical_entropy = self.cal_entropy(datasets)</span><br><span class="line">    best_feature = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> range(feature_count):</span><br><span class="line">        c_info_gain = self.info_gain(empirical_entropy, </span><br><span class="line">                                     self.cal_conditional_entropy(datasets, axis=c))</span><br><span class="line">        best_feature.append((c, c_info_gain))</span><br><span class="line"></span><br><span class="line">    best = max(best_feature, key=<span class="keyword">lambda</span> x : x[<span class="number">-1</span>])</span><br><span class="line">    <span class="comment"># best : ((feature_id, feature_info_gain))</span></span><br><span class="line">    <span class="keyword">return</span> best</span><br></pre></td></tr></table></figure></p>
<p>Define the class Node as : <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, splitting_feature_id=None, class_label=None, data=None, </span></span></span><br><span class="line"><span class="function"><span class="params">                 splitting_feature_value=None)</span>:</span></span><br><span class="line">        self.splitting_feature_id = splitting_feature_id         <span class="comment"># splitting feature</span></span><br><span class="line">        self.splitting_feature_value = splitting_feature_value   <span class="comment"># splitting feature value</span></span><br><span class="line">        self.class_label = class_label                           <span class="comment"># class label, only leaf has</span></span><br><span class="line">        self.data = data                                         <span class="comment"># labels of samples, only leaf has</span></span><br><span class="line">        self.child = []                                          <span class="comment"># child node</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_node</span><span class="params">(self, node)</span>:</span></span><br><span class="line">        self.child.append(node)</span><br></pre></td></tr></table></figure></p>
<p>Then achieve the ID3 algorithm : <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, train_data, node)</span>:</span></span><br><span class="line">        _ = train_data.iloc[:, :<span class="number">-1</span>]</span><br><span class="line">        y_train = train_data.iloc[:, <span class="number">-1</span>]</span><br><span class="line">        features = train_data.columns[:<span class="number">-1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1. if all the data in D belong to the same class C, </span></span><br><span class="line">        <span class="comment">#    set T as single node and use C as the label, return T</span></span><br><span class="line">        <span class="keyword">if</span> len(y_train.value_counts()) == <span class="number">1</span>:</span><br><span class="line">            node.class_label = y_train.iloc[<span class="number">0</span>]</span><br><span class="line">            node.data = y_train</span><br><span class="line">            <span class="keyword">return</span> </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2. if feature A is empty, set T as single node and use the label, </span></span><br><span class="line">        <span class="comment">#   most C as the label, return T</span></span><br><span class="line">        <span class="keyword">if</span> len(features) == <span class="number">0</span>:</span><br><span class="line">            node.class_label = y_train.value_counts().sort_values(ascending=<span class="literal">False</span>).index[<span class="number">0</span>]</span><br><span class="line">            node.data = y_train</span><br><span class="line">            <span class="keyword">return</span> </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3. calculate the largest inforamtion gain, use Ag to represents the best feature</span></span><br><span class="line">        max_feature_id, max_info_gain = self.get_info_gain(np.array(train_data))</span><br><span class="line">        max_feature_name = features[max_feature_id]</span><br><span class="line">     </span><br><span class="line">        <span class="comment"># 4. if the information gain is smaller than threshold, set T as single node,</span></span><br><span class="line">        <span class="comment">#    and use the most C as the label, return T </span></span><br><span class="line">        <span class="keyword">if</span> max_info_gain &lt;= self.epsilon:</span><br><span class="line">            node.class_label = y_train.value_counts().sort_values(ascending=<span class="literal">False</span>).index[<span class="number">0</span>]</span><br><span class="line">            node.data = y_train</span><br><span class="line">            <span class="keyword">return</span> </span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">        <span class="comment"># 5. splitting D according to each possible values in the feature A</span></span><br><span class="line">        feature_list = train_data[max_feature_name].value_counts().index</span><br><span class="line">        <span class="keyword">for</span> Di <span class="keyword">in</span> feature_list:</span><br><span class="line">            node.splitting_feature_id = max_feature_id</span><br><span class="line">            child = Node(splitting_feature_value = Di)</span><br><span class="line">            node.add_node(child)</span><br><span class="line">            sub_train_data = pd.DataFrame([list(i) <span class="keyword">for</span> i <span class="keyword">in</span> train_data.values <span class="keyword">if</span> i[max_feature_id] == Di],</span><br><span class="line">                                         columns = train_data.columns)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 6. create tree recursively</span></span><br><span class="line">            self.train(sub_train_data, child)</span><br></pre></td></tr></table></figure></p>
<h2 id="c4.5-algorithm">C4.5 Algorithm</h2>
<p>The C4.5 algorithm becomes extremely easy to understand after introducing the ID3 algorithm, cause there's only one different point between the two methods, that is C4.5 algorithm uses <strong>information gain ratio</strong> to choose the splitting feature instead of using information gain, besides this, other steps in C4.5 are same with those in ID3 algorithm.</p>
<h1 id="decision-tree-pruning">Decision Tree Pruning</h1>
<h2 id="a-simple-pruning-approach">A Simple Pruning approach</h2>
<p>In this part, we just discuss the pruning algorithm for ID3 and C4.5, the pruning for CART will be interpreted in the next part.</p>
<p>We have discussed the decision tree generating approached in the above part, however, it can overfit easily if there're lots of levels in the decision tree, so we need to prune the tree to avoid overfitting and simplify the process of calculating. Specifically, we cut some leaf nodes or sub-tree from the original tree and use their parents nodes as the new leaf nodes.</p>
<p>Like the other machine learning algorithms, here we prune the decision tree by minimising the loss function. Assume that <span class="math inline">\(|T|\)</span> is the number of leaf nodes, <span class="math inline">\(t\)</span> represents the leaf nodes and there are <span class="math inline">\(N_t\)</span> samples in it, use <span class="math inline">\(N_{tk}\)</span> to denote the number of samples which belong to class <span class="math inline">\(k\)</span> in <span class="math inline">\(N_t\)</span>, where <span class="math inline">\(k=1,2,...,K\)</span> ; <span class="math inline">\(H_t(T)\)</span> is the empirical entropy of leaf node <span class="math inline">\(t\)</span>, then define the loss function as : <span class="math display">\[C_\alpha(T) = \sum_{t=1}^{|T|}N_t*H_t(T) + \alpha|T|\]</span> We define the training error as : <span class="math display">\[C(T) = \sum_{t=1}^{|T|}N_t*H_t(T) = -\sum_{t=1}^{|T|}\sum_{k=1}^KN_{tk} * log {N_{tk}\over N_t}\]</span> In the above formula, the term <span class="math inline">\(|T|\)</span> which the number of leaf nodes, can represent the complexity of decision tree, the parameter <span class="math inline">\(\alpha\)</span>, can be understood as the coefficient of regularization, so the term <span class="math inline">\(\alpha|T|\)</span> actually has the same function with regularization term, which can find the trade-off between the complexity and precision of model. A larger <span class="math inline">\(\alpha\)</span> can promote to choose a simple model and a smaller <span class="math inline">\(\alpha\)</span> promote to choose a complex model.</p>
<p>Our pruning algorithm is going to find the sub-tree with the minimum loss function, here we calculate the loss value <span class="math inline">\(C_\alpha(T)\)</span> for the current tree, then calculating it again after cutting the brunch (sub-tree) or leaf node, now we get two different information gain values, record as <span class="math inline">\(C_\alpha(T_B)\)</span> and <span class="math inline">\(C_\alpha(T_A)\)</span>, so we can determine whether it's worth to cut this brunch or not by comparing the two error values, the specific steps are :</p>
<ol type="1">
<li><p>Calculating the empirical entropy for each leaf nodes;</p></li>
<li><p>Going back to the parents nodes recursively, cutting the brunch and comparing the error values between before cutting and after cutting, if <span class="math inline">\(C_\alpha(T_A) \leq C_\alpha(T_B)\)</span>, saving the pruning action and setting the parent node as the new leaf node, otherwise, recover the original tree;</p></li>
<li><p>Repeating the step 2 until all the nodes have been checked, then get the sub-tree with the minimum loss function</p></li>
</ol>
<h2 id="code-1">Code</h2>
<p>First, we need to achieve the loss function which is the <span class="math inline">\(C_\alpha(T)\)</span> in the above formula : <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># calculate C_alpha_T for current sub-tree</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">c_error</span><span class="params">(self)</span>:</span>   </span><br><span class="line">    leaf = []</span><br><span class="line">    self.find_leaf(self.tree, leaf)</span><br><span class="line">    <span class="comment"># count the N_t, len(leaf_num) == |T|</span></span><br><span class="line">    leaf_num = [len(l) <span class="keyword">for</span> l <span class="keyword">in</span> leaf]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># calculate empirical entropy for each leaf nodes</span></span><br><span class="line">    entropy = [self.cal_entropy(l) <span class="keyword">for</span> l <span class="keyword">in</span> leaf]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># alpha * |T|</span></span><br><span class="line">    alpha_T = self.alpha * len(leaf_num)</span><br><span class="line">    </span><br><span class="line">    error = <span class="number">0</span></span><br><span class="line">    C_alpha_T = <span class="number">0</span> + alpha_T</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> Nt, Ht <span class="keyword">in</span> zip(leaf_num, entropy):</span><br><span class="line">        C_T = Nt * Ht</span><br><span class="line">        error += C_T</span><br><span class="line">        </span><br><span class="line">    C_alpha_T += error   </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> C_alpha_T</span><br><span class="line">   </span><br><span class="line"><span class="comment"># find all leaf nodes   </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_leaf</span><span class="params">(self, node, leaf)</span>:</span>   </span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> node.child:</span><br><span class="line">        <span class="keyword">if</span> t.class_label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            leaf.append(t.data)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> node.child:</span><br><span class="line">                self.find_leaf(c, leaf)</span><br></pre></td></tr></table></figure></p>
<p>Then calculate loss value for the original tree, starting pruning : <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pruning</span><span class="params">(self, alpha=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> alpha:</span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        </span><br><span class="line">    error_min = self.c_error()</span><br><span class="line">    </span><br><span class="line">    self.find_parent(self.tree, error_min)</span><br></pre></td></tr></table></figure></p>
<p>In which the <strong>find_parent</strong> function corresponds the <strong>step 2</strong> in the above principle introduction part : <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_parent</span><span class="params">(self, node, error_min)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    leaf nodes : class_label -&gt; not None</span></span><br><span class="line"><span class="string">                 data -&gt; not None</span></span><br><span class="line"><span class="string">                 splitting_feature_id -&gt; None</span></span><br><span class="line"><span class="string">                 splitting_feature_value -&gt; None</span></span><br><span class="line"><span class="string">                 child -&gt; None</span></span><br><span class="line"><span class="string">    ---------------------------------------------</span></span><br><span class="line"><span class="string">    other nodes: class_label -&gt; None</span></span><br><span class="line"><span class="string">                 data -&gt; None</span></span><br><span class="line"><span class="string">                 splitting_feature_id -&gt; not None</span></span><br><span class="line"><span class="string">                 splitting_feature_value -&gt; not None(except root)</span></span><br><span class="line"><span class="string">                 child -&gt; not None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># if not the leaf nodes</span></span><br><span class="line">    <span class="keyword">if</span> node.splitting_feature_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:  </span><br><span class="line">        <span class="comment"># collect class_labels from child nodes</span></span><br><span class="line">        class_label = [c.class_label <span class="keyword">for</span> c <span class="keyword">in</span> node.child]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># if all the child nodes are leaf nodes</span></span><br><span class="line">        <span class="keyword">if</span> <span class="literal">None</span> <span class="keyword">not</span> <span class="keyword">in</span> class_label:  </span><br><span class="line">            <span class="comment"># collect data from child nodes</span></span><br><span class="line">            child_data = []</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> node.child:</span><br><span class="line">                <span class="keyword">for</span> d <span class="keyword">in</span> list(c.data):</span><br><span class="line">                    child_data.append(d)</span><br><span class="line">            child_counter = Counter(child_data)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># copy the old node</span></span><br><span class="line">            old_child = node.child</span><br><span class="line">            old_splitting_feature_id = node.splitting_feature_id</span><br><span class="line">            old_class_label = node.class_label</span><br><span class="line">            old_data = node.data</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># pruning</span></span><br><span class="line">            node.splitting_feature_id = <span class="literal">None</span></span><br><span class="line">            node.class_label = child_counter.most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">            node.data = child_data</span><br><span class="line">            </span><br><span class="line">            error_after_pruning = self.c_error()</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># if error_after_pruning &lt;= error_min, it is worth to pruning</span></span><br><span class="line">            <span class="keyword">if</span> error_after_pruning &lt;= error_min:</span><br><span class="line">                error_min = error_after_pruning</span><br><span class="line">                <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">                </span><br><span class="line">            <span class="comment"># if not, recover the previous tree</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                node.child = old_child</span><br><span class="line">                node.splitting_feature_id = old_splitting_feature_id</span><br><span class="line">                node.class_label = old_class_label</span><br><span class="line">                node.data = old_data</span><br><span class="line">                </span><br><span class="line">        <span class="comment"># if not all the child nodes are leaf nodes</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            re = <span class="number">0</span></span><br><span class="line">            i = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> i &lt; len(node.child):</span><br><span class="line">                <span class="comment"># if the pruning action happend, </span></span><br><span class="line">                <span class="comment"># rescan the sub-tree since some new leaf nodes are created</span></span><br><span class="line">                if_re = self.find_parent(node.child[i], error_min)</span><br><span class="line">                <span class="keyword">if</span> if_re == <span class="number">1</span>:   </span><br><span class="line">                    re = <span class="number">1</span></span><br><span class="line">                <span class="keyword">elif</span> if_re == <span class="number">2</span>:  </span><br><span class="line">                    i -= <span class="number">1</span></span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> re:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<h1 id="cart">CART</h1>
<p>As the name of this algorithm -- classification and regression tree, it can apply on both classification and regression, actually the essence of regression tree is based on classification principle, I'm going to explain it later.</p>
<h2 id="regression-tree">Regression Tree</h2>
<h3 id="principle-of-generating">Principle of Generating</h3>
<p>The process of generating regression tree is also the procedure of establishing binary decision tree recursively by minimising the mean square error.</p>
<p>In the regression problems, we assume the output <span class="math inline">\(Y\)</span> is continuous variable, so the training set is : <span class="math display">\[D = \{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}\]</span> Assume that we have splitted the data set into <span class="math inline">\(M\)</span> units: <span class="math inline">\(R_1, R2,...,R_M\)</span> , and there are one unique output value in each units <span class="math inline">\(R_m\)</span>, record as <span class="math inline">\(c_m\)</span>, in other word, we split the training set into plenty of units, and calculate an output or prediction value for each subsets, therefore, the model can be represented as : <span class="math display">\[f(x) = \sum_{m=1}^Mc_m\]</span> And we use the mean square error to evaluate the training error for our model : <span class="math display">\[\sum(y_i-f(x_i))^2\]</span> In addition, the optimal output value of <span class="math inline">\(R_m\)</span> is : <span class="math display">\[c_m = average(y_i|x_i \in R_m)\]</span> Now, we need to choose the appropriate feature and value to split the data set, we call the chosen feature as <strong>splitting variable</strong> and the value is <strong>splitting point</strong>. Actually there's no sample or convenient method to find these two value, what we can do is go through all the features and for possible values of each features, calculate the summary error of the two subsets (units) which are splitted based on the values. The detailed steps are:</p>
<ol type="1">
<li><p>For the all features, scan possible values, find the specific feature <span class="math inline">\(j\)</span> and value <span class="math inline">\(s\)</span> which can guarantee the below formula reaches at the minimum value : <span class="math display">\[\min_{j,s} \{ \min_{c_1}\sum_{x_i\in R_1}(y_i-c_1)^2 + \min_{c_2}\sum_{x_i\in R_2}(y_i-c_2)^2 \}\]</span></p></li>
<li><p>For the specific <span class="math inline">\((j,s)\)</span>, splitting the data set and calculate the outputs : <span class="math display">\[R_1(j,s)=\{x|x^{(j)}\leq s\}, R_2(j,s)=\{x|x^{(j)}\geq s\}\]</span> <span class="math display">\[c_m={1 \over N_m} * \sum_{x_i\in R_m}y_i, (x\in R_m, m=1,2)\]</span></p></li>
<li><p>Repeat the first two steps recursively, until the deep of level reaches at the previous setting</p></li>
<li><p>Then we can get the decision tree model : <span class="math display">\[f(x) = \sum_{m=1}^Mc_m\]</span></p></li>
</ol>
<h3 id="code-2">Code</h3>
<p>First, we define the Node class for the tree as : <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, splitting_name=None, splitting_value=None, c=None)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        leaf :  splitting_name --&gt; None</span></span><br><span class="line"><span class="string">                splitting_value --&gt; None</span></span><br><span class="line"><span class="string">                child --&gt; None</span></span><br><span class="line"><span class="string">                c --&gt; Not None</span></span><br><span class="line"><span class="string">        -------------------------------</span></span><br><span class="line"><span class="string">        others: splitting_name --&gt; Not None</span></span><br><span class="line"><span class="string">                splitting_value --&gt; Not None</span></span><br><span class="line"><span class="string">                child --&gt; Not None</span></span><br><span class="line"><span class="string">                c --&gt; Not None</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.splitting_name = splitting_name</span><br><span class="line">        self.splitting_value = splitting_value</span><br><span class="line">        self.child = []</span><br><span class="line">        self.c = c</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_node</span><span class="params">(self, node)</span>:</span></span><br><span class="line">        self.child.append(node)</span><br></pre></td></tr></table></figure></p>
<p>The mean square error method and the optimal <span class="math inline">\(c_m\)</span> calculation : <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mse</span><span class="params">(self, r, c)</span>:</span></span><br><span class="line">    y = np.array(r[<span class="string">'target'</span>])</span><br><span class="line">    error = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> y:</span><br><span class="line">        error += np.power((i - c), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> error</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_best_c</span><span class="params">(self, r)</span>:</span></span><br><span class="line">    <span class="comment"># target is the label feature</span></span><br><span class="line">    y = r[<span class="string">'target'</span>]</span><br><span class="line">    <span class="keyword">return</span> np.mean(y)</span><br></pre></td></tr></table></figure></p>
<p>As for the training part, achieving the above first two steps recursively : <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, dataset, node, max_depth, depth)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    @ dataset: the training data</span></span><br><span class="line"><span class="string">    @ node: nodes of the tree</span></span><br><span class="line"><span class="string">    @ max_depth: the total deep of the tree</span></span><br><span class="line"><span class="string">    @ depth: current depth</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    train_data = dataset.iloc[:, <span class="number">0</span>:<span class="number">-1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># if the data set can't be splitted, return </span></span><br><span class="line">    <span class="keyword">if</span> train_data.shape[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># create two child nodes for each non-leaf nodes</span></span><br><span class="line">    child1 = Node()</span><br><span class="line">    child2 = Node()</span><br><span class="line">    node.add_node(child1)</span><br><span class="line">    node.add_node(child2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initial the error as infinity</span></span><br><span class="line">    error_sum = float(<span class="string">'inf'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># go through the features' values, find the j,s</span></span><br><span class="line">    <span class="keyword">for</span> feature_name <span class="keyword">in</span> train_data.columns.values:</span><br><span class="line">        <span class="keyword">for</span> feature_value <span class="keyword">in</span> train_data[feature_name]:</span><br><span class="line">            <span class="comment"># split the data set into two units</span></span><br><span class="line">            r1_list = [rows <span class="keyword">for</span> index, rows <span class="keyword">in</span> dataset.iterrows() </span><br><span class="line">                            <span class="keyword">if</span> rows[feature_name] &lt; feature_value]</span><br><span class="line">            </span><br><span class="line">            r2_list = [rows <span class="keyword">for</span> index, rows <span class="keyword">in</span> dataset.iterrows() </span><br><span class="line">                            <span class="keyword">if</span> rows[feature_name] &gt;= feature_value]</span><br><span class="line">            </span><br><span class="line">            r1 = pd.DataFrame(r1_list, columns=dataset.columns)</span><br><span class="line">            c1 = self.get_best_c(r1)</span><br><span class="line">            error1 = self.mse(r1, c1)</span><br><span class="line">            </span><br><span class="line">            r2 = pd.DataFrame(r2_list, columns=dataset.columns)</span><br><span class="line">            c2 = self.get_best_c(r2)</span><br><span class="line">            error2 = self.mse(r2, c2)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> (error1 + error2) &lt; error_sum:</span><br><span class="line">                error_sum = error1 + error2</span><br><span class="line">                node.splitting_name = feature_name</span><br><span class="line">                node.splitting_value = feature_value</span><br><span class="line">                child1.c = c1</span><br><span class="line">                child2.c = c2</span><br><span class="line">    </span><br><span class="line">    print(depth, node.splitting_name, node.splitting_value)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># delete the splitting variable</span></span><br><span class="line">    r1 = r1.drop(node.splitting_name, axis=<span class="number">1</span>)</span><br><span class="line">    r2 = r2.drop(node.splitting_name, axis=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># if has arrived at the last level, return </span></span><br><span class="line">    <span class="keyword">if</span> depth == max_depth:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">         </span><br><span class="line">    <span class="comment"># repeat the above actions for each childs</span></span><br><span class="line">    self.train(r1, child1, max_depth, depth+<span class="number">1</span>)</span><br><span class="line">    self.train(r2, child2, max_depth, depth+<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span></span><br></pre></td></tr></table></figure></p>
<h3 id="the-idea-of-classification">The Idea of Classification</h3>
<p>As we said, the essence of regression tree is based on the idea of classification principle, and now I'm going to give the interpretation.</p>
<p>In the above part, we mentioned that using the average value of the labels as the output in each subsets (units) which means we seem the each units as a 'class', and the class label is the output <span class="math inline">\(c_m\)</span>. When we need to predict for a new data item, the model just need to classify it according to the splitting variable and splitting point which we found previously, therefore, when the model classify the data into a leaf node, the output <span class="math inline">\(c_m\)</span> of that leaf node is our prediction value.</p>
<p>Here we use a graph to help us to understand it better : <img src="/images/decision_tree/decision_tree_1.png"></p>
<p>As we see, when the max_depth is equal to 1, the data is approximately splitted into two parts, and be classified into more classes when the value of max_depth is 3. Therefore, in fact, the regression tree use the idea of classification to classify the 'similar' samples as a specific class, and use the average value of their labels as the class label, in this way a regression model has achieved.</p>
<h2 id="classification-tree">Classification Tree</h2>
<h3 id="generating-principle">Generating Principle</h3>
<p>The core concept of generating of classification tree is using the <strong>Gini Index</strong> to select the optimal features and determine the optimal the binary value splitting point in each levels.</p>
<p>Assume that we have <span class="math inline">\(K\)</span> classes in total, and the probability of sample belongs to the class <span class="math inline">\(k\)</span> is <span class="math inline">\(p_k\)</span>, so that the <strong>Gini Index</strong> can be defined as : <span class="math display">\[Gini(p) = \sum_{k=1}^K p_k * (1-p_k) = 1 - \sum_{k=1}^K p_k^2\]</span> And for the binary classification problem, we have : <span class="math display">\[Gini(p) = 2p * (1-p)\]</span> For a given data set <span class="math inline">\(D\)</span>, there has : <span class="math display">\[Gini(D) = 1 - \sum_{k=1}^K ({|C_k| \over |D|})^2\]</span> In which <span class="math inline">\(C_k\)</span> is the subset of <span class="math inline">\(D\)</span> which the samples belong to the class <span class="math inline">\(k\)</span>, and <span class="math inline">\(K\)</span> represents the number of classes.</p>
<p>If we split the data set into two parts <span class="math inline">\(D_1\)</span> and <span class="math inline">\(D_2\)</span> according to whether the value of feature <span class="math inline">\(A\)</span> is equal to the possible value <span class="math inline">\(a\)</span>, then the gini index in this situation is : <span class="math display">\[Gini(D,A) = {|D_1|\over |D|} * Gini(D_1) + {|D_2|\over |D|} * Gini(D_2)\]</span> Where like the entropy, gini index can also represent the uncertainty of data set, and <span class="math inline">\(Gini(D,A)\)</span> represents the uncertainty of data set which is splitted by <span class="math inline">\(A=a\)</span>, in addition, still like the entropy, a large gini index value means the large uncertainty.</p>
<p>The below steps show the detailed process of classification tree generating :</p>
<ol type="1">
<li><p>For each feature <span class="math inline">\(A\)</span>, and for its each possible value <span class="math inline">\(a\)</span>, splitting the data set <span class="math inline">\(D\)</span> into two parts <span class="math inline">\(D_1\)</span> and <span class="math inline">\(D_2\)</span> according to whether <span class="math inline">\(A=a\)</span>, calculate the value of <span class="math inline">\(Gini(D, A)\)</span>;</p></li>
<li><p>For each feature and its possible value, select the optimal feature <span class="math inline">\(A\)</span> and the splitting point <span class="math inline">\(a\)</span> which can make <span class="math inline">\(Gini(D, A)\)</span> has the minimum value, and distribute data set to <span class="math inline">\(D_1\)</span> and <span class="math inline">\(D_2\)</span> respectively;</p></li>
<li><p>For the sub-nodes, repeat the first two steps recursively until there's no further features or reaches at the max depth, or the number of samples in node is less than the threshold which we set previously.</p></li>
</ol>
<h3 id="code-3">Code</h3>
<p>First, we define the Node class as : <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, splitting_feature=None, splitting_point=None, class_label=None, label_data=None)</span>:</span></span><br><span class="line">        self.splitting_feature = splitting_feature</span><br><span class="line">        self.splitting_point = splitting_point</span><br><span class="line">        self.child = []</span><br><span class="line">        self.class_label = class_label</span><br><span class="line">        self.label_data = label_data  <span class="comment"># store the labels of the samples</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_child</span><span class="params">(self, node)</span>:</span></span><br><span class="line">        self.child.append(node)</span><br></pre></td></tr></table></figure></p>
<p>Then achieve the gini index calculation : <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gini_index</span><span class="params">(self, data, A, a)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    @ A: splitting vairable (feature)</span></span><br><span class="line"><span class="string">    @ a: splitting point (possible value of A)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># splitting the data set accroding to whether A == a</span></span><br><span class="line">    D1 = data.loc[data[A] == a]</span><br><span class="line">    D2 = data.loc[data[A] != a]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># count the value of |C_k| respectively</span></span><br><span class="line">    D1_label_count = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(D1.shape[<span class="number">0</span>]):</span><br><span class="line">        label = D1.iloc[i, <span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">not</span> <span class="keyword">in</span> D1_label_count:</span><br><span class="line">            D1_label_count[label] = <span class="number">0</span></span><br><span class="line">        D1_label_count[label] += <span class="number">1</span> </span><br><span class="line">    </span><br><span class="line">    D2_label_count = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(D2.shape[<span class="number">0</span>]):</span><br><span class="line">        label = D2.iloc[i, <span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">not</span> <span class="keyword">in</span> D2_label_count:</span><br><span class="line">            D2_label_count[label] = <span class="number">0</span></span><br><span class="line">        D2_label_count[label] += <span class="number">1</span> </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># calculate the gini index</span></span><br><span class="line">    gini_D1 = (D1.shape[<span class="number">0</span>] / data.shape[<span class="number">0</span>]) * sum([<span class="number">1</span> - c_k/D1.shape[<span class="number">0</span>] <span class="keyword">for</span> c_k <span class="keyword">in</span> D1_label_count.values()])</span><br><span class="line">    gini_D2 = (D2.shape[<span class="number">0</span>] / data.shape[<span class="number">0</span>]) * sum([<span class="number">1</span> - c_k/D2.shape[<span class="number">0</span>] <span class="keyword">for</span> c_k <span class="keyword">in</span> D2_label_count.values()])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gini_D1 + gini_D2</span><br></pre></td></tr></table></figure></p>
<p>And achieve the steps in the last part to generate the classification tree : <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, data, threshold)</span>:</span></span><br><span class="line">    self.train(data, self.tree, threshold)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, data, node, threshold)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    leaf nodes: splitting_feature --&gt; None</span></span><br><span class="line"><span class="string">                splitting_point --&gt; None</span></span><br><span class="line"><span class="string">                child --&gt; None</span></span><br><span class="line"><span class="string">                class_label --&gt; not None</span></span><br><span class="line"><span class="string">                label_data --&gt; not None</span></span><br><span class="line"><span class="string">    ----------------------------------------</span></span><br><span class="line"><span class="string">    others :    splitting_feature --&gt; not None</span></span><br><span class="line"><span class="string">                splitting_point --&gt; not None</span></span><br><span class="line"><span class="string">                child --&gt; not None</span></span><br><span class="line"><span class="string">                class_label --&gt; None</span></span><br><span class="line"><span class="string">                label_data --&gt; None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    labels = data.iloc[:, <span class="number">-1</span>]</span><br><span class="line">    train_data = data.iloc[:, <span class="number">0</span>:<span class="number">-1</span>]</span><br><span class="line">    features_list = train_data.columns.values</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># if the number of samples less than the threshold, </span></span><br><span class="line">    <span class="comment"># setting it as the leaf node and return </span></span><br><span class="line">    <span class="keyword">if</span> len(data) &lt; threshold:</span><br><span class="line">        <span class="comment"># use the most class among samples as the class label</span></span><br><span class="line">        node.class_label = labels.value_counts().sort_values(ascending=<span class="literal">False</span>).index[<span class="number">0</span>]</span><br><span class="line">        node.label_data = labels</span><br><span class="line">        <span class="keyword">return</span> </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># if all the samples are belong to the same class,</span></span><br><span class="line">    <span class="comment"># setting it as the leaf node and return</span></span><br><span class="line">    <span class="keyword">if</span> len(labels.value_counts()) == <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># use the label of samples as the class label</span></span><br><span class="line">        node.class_label = labels.iloc[<span class="number">0</span>]</span><br><span class="line">        node.label_data = labels</span><br><span class="line">        <span class="keyword">return</span> </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># if there' no data in the data set, just return</span></span><br><span class="line">    <span class="keyword">if</span> train_data.empty:</span><br><span class="line">        node.class_label = labels.value_counts().sort_values(ascending=<span class="literal">False</span>).index[<span class="number">0</span>]</span><br><span class="line">        node.label_data = labels</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize the gini index as positive infinity</span></span><br><span class="line">    gini = float(<span class="string">"inf"</span>)</span><br><span class="line">    <span class="keyword">for</span> A <span class="keyword">in</span> features_list:</span><br><span class="line">        <span class="keyword">for</span> a <span class="keyword">in</span> train_data[A]:</span><br><span class="line">            gini_c = self.gini_index(data, A, a)</span><br><span class="line">            <span class="keyword">if</span> gini_c &lt; gini:</span><br><span class="line">                feature = A</span><br><span class="line">                point = a</span><br><span class="line">                gini = gini_c</span><br><span class="line"></span><br><span class="line">    node.splitting_feature = feature</span><br><span class="line">    node.splitting_point = point</span><br><span class="line">    node.add_child(Node())</span><br><span class="line">    node.add_child(Node())</span><br><span class="line">    </span><br><span class="line">    D1 = data.loc[data[node.splitting_feature] == node.splitting_point].drop(node.splitting_feature, axis=<span class="number">1</span>)</span><br><span class="line">    D2 = data.loc[data[node.splitting_feature] != node.splitting_point].drop(node.splitting_feature, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    self.train(D1, node.child[<span class="number">0</span>], threshold)</span><br><span class="line">    self.train(D2, node.child[<span class="number">1</span>], threshold)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span></span><br></pre></td></tr></table></figure></p>
<h2 id="cart-pruning">CART Pruning</h2>
<h3 id="pruning-algorithm">Pruning Algorithm</h3>
<p>Like the pruning algorithm in ID3 and C4.5, here we also use loss function as the evaluation method, as above, the loss function is defined as : <span class="math display">\[C_\alpha(T) = C(T) + \alpha|T|\]</span> In which we call <span class="math inline">\(\alpha\)</span> as the parameter of regularization. <span class="math inline">\(C(T)\)</span> represents the training error (here we use gini index, we used the mse in regression tree). A large <span class="math inline">\(\alpha\)</span> promotes the algorithm to choice a simpler model and a small <span class="math inline">\(\alpha\)</span> will generate a more complex tree.</p>
<p>Since we can find the unique optimal sub-tree <span class="math inline">\(T_\alpha\)</span> with a given <span class="math inline">\(\alpha\)</span>, therefore, we can prune the original tree recursively : we can define a series <span class="math inline">\(\alpha\)</span>, such as <span class="math inline">\(\alpha_0&lt;\alpha_1&lt;\alpha_2&lt;...&lt;\alpha_n\)</span>, and we can find the corresponding optimal sub-tree <span class="math inline">\(\{T_0,T_1,T_2,...,T_n\}\)</span>.</p>
<p>Specifically, assume that <span class="math inline">\(t\)</span> is a node in original tree, and the loss function for the single node tree which <span class="math inline">\(t\)</span> is the root node is : <span class="math display">\[C_\alpha(t) = C(t) + \alpha\]</span> The loss function for sub-tree which <span class="math inline">\(t\)</span> is the root node is : <span class="math display">\[C_\alpha(T_t) = C(T_t) + \alpha|T_t|\]</span> If <span class="math inline">\(\alpha = 0\)</span> or <span class="math inline">\(\alpha\)</span> is extremely small, we have : <span class="math display">\[C_\alpha(T_t) &lt; C_\alpha(t)\]</span> When <span class="math inline">\(\alpha\)</span> increased, we have the below equation with the specific <span class="math inline">\(\alpha\)</span> : <span class="math display">\[C_\alpha(T_t) = C_\alpha(t)\]</span> Increasing <span class="math inline">\(\alpha\)</span> continually, so we can get : <span class="math display">\[C_\alpha(T_t) &gt; C_\alpha(t)\]</span> And when <span class="math inline">\(C_\alpha(T_t) = C_\alpha(t)\)</span>, we have : <span class="math display">\[ C(t) + \alpha = C(T_t) + \alpha|T_t|\]</span> <span class="math display">\[\alpha = {C(t) - C(T_t) \over |T_t| - 1}\]</span> Where <span class="math inline">\(t\)</span> has the same value of loss function with <span class="math inline">\(T_t\)</span>, and there are less nodes in <span class="math inline">\(t\)</span>, therefore, we choice <span class="math inline">\(t\)</span> instead of <span class="math inline">\(T_t\)</span>, which can be seen as the action of pruning.</p>
<p>So we can calculate the below equation for each internal nodes in the original tree ; <span class="math display">\[g(t) = {C(t) - C(T_t) \over |T_t| - 1}\]</span> Which represents the degree of reduction of loss function after pruning, it means when the given <span class="math inline">\(\alpha &lt; g(t)\)</span>, pruning will increase the total loss function; if <span class="math inline">\(\alpha &gt; g(t)\)</span>, pruning can decrease the total loss function.</p>
<p>We choice the smallest value of <span class="math inline">\(g(t)\)</span> in each step, set that <span class="math inline">\(g(t)\)</span> as <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(T_i\)</span> is the optimal sub-tree in the interval <span class="math inline">\([\alpha_i, \alpha_{i+1})\)</span>. Pruning the tree until reach at the root node, we need to increase the value of <span class="math inline">\(\alpha\)</span> in the process. Below steps show the process of the pruning :</p>
<ol type="1">
<li><p>Set <span class="math inline">\(k=0\)</span>, <span class="math inline">\(T=T_0\)</span>, <span class="math inline">\(\alpha=+\infty\)</span></p></li>
<li><p>For each interval nodes <span class="math inline">\(t\)</span>, calculate <span class="math inline">\(g(t)\)</span> : <span class="math display">\[g(t) = {C(t) - C(T_t) \over |T_t| - 1}\]</span> <span class="math display">\[\alpha = min(\alpha, g(t))\]</span></p></li>
<li><p>Prune the sub-tree <span class="math inline">\(t\)</span> which meets the requirement <span class="math inline">\(g(t) = \alpha\)</span>, set <span class="math inline">\(t\)</span> as the leaf node and use the most labels in <span class="math inline">\(t\)</span> as the class label</p></li>
<li><p>Set <span class="math inline">\(k=k+1\)</span>, <span class="math inline">\(\alpha_k=\alpha\)</span>, <span class="math inline">\(T_k=T\)</span></p></li>
<li><p>Repeat the steps 3 and 4 until there is only a root node with two leaf nodes in the tree</p></li>
<li><p>Choice the best sub-tree in the set <span class="math inline">\(\{T_0,T_1,...,T_n\}\)</span> by using cross validation</p></li>
</ol>
<p><strong>In conclusion, the main idea of CART pruning is calculating the value of <span class="math inline">\(g(t)\)</span> for each interval nodes, which represents the condition that it's worth to cut the brunch, choice the smallest <span class="math inline">\(g(t)\)</span> as <span class="math inline">\(\alpha_i\)</span>. And cut the sub-tree where <span class="math inline">\(g(t)=\alpha\)</span>, which means every time we cut the sub-trees which have the smallest <span class="math inline">\(g(t)\)</span>, in this way, with the value of <span class="math inline">\(\alpha\)</span> increasing (cause every time we choice the smallest <span class="math inline">\(g(t)\)</span> for <span class="math inline">\(\alpha\)</span>), there are always exist at least one sub-tree will be pruned, and for each interval <span class="math inline">\([\alpha_i, \alpha_{i+1})\)</span>, there is an unique sub-tree <span class="math inline">\(T_t\)</span> in corresponding. Repeat the pruning action until there are only root node with two leaf nodes in the original tree, we get a sub-tree set <span class="math inline">\(\{T_0,T_1,...,T_n\}\)</span> which records all the sub-trees in each pruning step, then select the best tree by cross validation method.</strong></p>
<p>In addition, I'm going to interpret why we choice the smallest <span class="math inline">\(g(t)\)</span> as <span class="math inline">\(\alpha_i\)</span> and prune that brunch in each step. Since we are going to select the best sub-tree by using cross validation in the final step, here we don't want to miss any possible sub-tree, as we said, a large <span class="math inline">\(\alpha\)</span> means we want the algorithm to choice a simpler model which in decision tree corresponding to a small sized tree, so when we choice the smallest <span class="math inline">\(g(t)\)</span> (also the <span class="math inline">\(\alpha\)</span>) in each step, it's also the process of increasing the value of <span class="math inline">\(\alpha\)</span>, therefore, we can get a set with sub-tree <span class="math inline">\(\{T_0,T_1,...,T_n\}\)</span> in the decreasing size order. In this way, it's easy to image that our action is cutting the brunches step by step until reach at the root node, it's also easy to interpret and achieve by coding. Actually, it can acquire the same result if we choice the largest <span class="math inline">\(g(t)\)</span> in each step, but it's not an algorithm with good-interpretation property.</p>
<h2 id="code-4">Code</h2>
<p>First is the function of finding all leaf nodes of the given node : <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_leaf</span><span class="params">(self, node, leaf)</span>:</span>    <span class="comment"># find all leaf nodes</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> node.child:</span><br><span class="line">        <span class="keyword">if</span> t.class_label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            leaf.append(t.label_data)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> node.child:</span><br><span class="line">                self.find_leaf(c, leaf)</span><br></pre></td></tr></table></figure></p>
<p>Then, the gini index achievement and <span class="math inline">\(g(t)\)</span> calculation : <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gini_pruning</span><span class="params">(self, leaf_nodes)</span>:</span></span><br><span class="line">    gini = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> leaf_nodes:</span><br><span class="line">        label_count = pd.value_counts(node)</span><br><span class="line">        gini_curr = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(label_count)):</span><br><span class="line">            gini_curr += math.pow((label_count[i]/len(node)), <span class="number">2</span>)</span><br><span class="line">        gini += <span class="number">1</span> - gini_curr</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> gini</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">g_t</span><span class="params">(self, node)</span>:</span></span><br><span class="line">    leaf_nodes = []</span><br><span class="line">    <span class="comment"># find all the leaf nodes</span></span><br><span class="line">    self.find_leaf(node, leaf_nodes)</span><br><span class="line">    <span class="comment"># |T_t|</span></span><br><span class="line">    T_t = len(leaf_nodes)</span><br><span class="line">    <span class="comment"># C(T_t)</span></span><br><span class="line">    C_T_t = self.gini_pruning(leaf_nodes)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># collect data labels from leaf nodes</span></span><br><span class="line">    labels = []</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> leaf_nodes:</span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> n:</span><br><span class="line">            labels.append(l)</span><br><span class="line">    <span class="comment"># C(t)    </span></span><br><span class="line">    C_t = self.gini_pruning(labels)</span><br><span class="line">    </span><br><span class="line">    gt = (C_t - C_T_t) / (T_t - <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gt</span><br></pre></td></tr></table></figure></p>
<p>Then we use the cut_brunch function to prune and iterate the tree : <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cut_brunch</span><span class="params">(self, node, pruning)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    leaf nodes: splitting_feature --&gt; None</span></span><br><span class="line"><span class="string">                splitting_point --&gt; None</span></span><br><span class="line"><span class="string">                child --&gt; None</span></span><br><span class="line"><span class="string">                class_label --&gt; not None</span></span><br><span class="line"><span class="string">                label_data --&gt; not None</span></span><br><span class="line"><span class="string">    ----------------------------------------</span></span><br><span class="line"><span class="string">    others :    splitting_feature --&gt; not None</span></span><br><span class="line"><span class="string">                splitting_point --&gt; not None</span></span><br><span class="line"><span class="string">                child --&gt; not None</span></span><br><span class="line"><span class="string">                class_label --&gt; None</span></span><br><span class="line"><span class="string">                label_data --&gt; None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    @ node : the given node</span></span><br><span class="line"><span class="string">    @ pruning : a flag to indicate whether the current</span></span><br><span class="line"><span class="string">                action is pruning or just go through </span></span><br><span class="line"><span class="string">                the tree to calculate the values of g(t)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># if is leaf node</span></span><br><span class="line">    <span class="keyword">if</span> node.splitting_feature <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="comment"># if is not leaf node</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># iterate the tree recursively, from bottom to top</span></span><br><span class="line">        self.cut_brunch(node.child[<span class="number">0</span>], pruning)</span><br><span class="line">        self.cut_brunch(node.child[<span class="number">1</span>], pruning)</span><br><span class="line">        <span class="comment"># calculate the value of g(t)</span></span><br><span class="line">        gt = self.g_t(node)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># if the action is pruning, find the </span></span><br><span class="line">        <span class="comment"># alpha = g(t) and cut the brunch</span></span><br><span class="line">        <span class="keyword">if</span> pruning:</span><br><span class="line">            <span class="keyword">if</span> gt == self.alpha:</span><br><span class="line">                <span class="comment"># collect the labels from leaf nodes</span></span><br><span class="line">                leaf_label = []</span><br><span class="line">                self.find_leaf(node, leaf_label)</span><br><span class="line">                labels = []</span><br><span class="line">                <span class="keyword">for</span> n <span class="keyword">in</span> leaf_label:</span><br><span class="line">                    <span class="keyword">for</span> l <span class="keyword">in</span> n:</span><br><span class="line">                        labels.append(l)</span><br><span class="line">                label_count = Counter(labels)</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># pruning</span></span><br><span class="line">                node.splitting_feature = <span class="literal">None</span></span><br><span class="line">                node.splitting_point = <span class="literal">None</span></span><br><span class="line">                node.child[<span class="number">0</span>] = <span class="literal">None</span></span><br><span class="line">                node.child[<span class="number">1</span>] = <span class="literal">None</span></span><br><span class="line">                node.label_data = labels</span><br><span class="line">                <span class="comment"># use the most labels in the child nodes as the class label</span></span><br><span class="line">                node.class_label = label_count.most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># add current tree T_t to the sub-tree list  </span></span><br><span class="line">                self.sub_tree.append(copy.deepcopy(self.tree))</span><br><span class="line">                </span><br><span class="line">        <span class="comment"># otherwise, the action is calculating the value of g(t)</span></span><br><span class="line">        <span class="comment"># update the value of alpha if g(t) &lt; alpha</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># alpha = min(alpha, g(t))</span></span><br><span class="line">            <span class="keyword">if</span> gt &lt; self.alpha:</span><br><span class="line">                self.alpha = gt   </span><br><span class="line">             </span><br><span class="line">        <span class="keyword">return</span></span><br></pre></td></tr></table></figure></p>
<p>Finally, define a function to achieve the process of the pruning algorithm : <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pruning</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># initalize the alpha as positive infity</span></span><br><span class="line">    self.alpha = float(<span class="string">'inf'</span>)</span><br><span class="line">    <span class="comment"># define the alpha and sub-tree list</span></span><br><span class="line">    alpha_set = []</span><br><span class="line">    self.sub_tree = []</span><br><span class="line">    <span class="comment"># add the first original tree to the list</span></span><br><span class="line">    self.sub_tree.append(copy.deepcopy(self.tree))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># repeat the pruning until there are only root node with two</span></span><br><span class="line">    <span class="comment"># leaf nodes in the tree</span></span><br><span class="line">    <span class="keyword">while</span> self.tree.child[<span class="number">0</span>].splitting_feature != <span class="literal">None</span> <span class="keyword">or</span> self.tree.child[<span class="number">1</span>].splitting_feature != <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># first, iterate the tree, calculate the value of g(t)</span></span><br><span class="line">        self.cut_brunch(self.tree, <span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># then, iterate the tree again, prune the brunch </span></span><br><span class="line">        <span class="comment"># where alpha = g(t)</span></span><br><span class="line">        self.cut_brunch(self.tree, <span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># add alpha into list and reset the 'new' alpha</span></span><br><span class="line">        alpha_set.append(self.alpha)</span><br><span class="line">        self.alpha = float(<span class="string">'inf'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># add the last sub-tree, which only includes a root node</span></span><br><span class="line">    <span class="comment"># with two leaf nodes</span></span><br><span class="line">    self.sub_tree.append(self.tree)</span><br></pre></td></tr></table></figure></p>
<h2 id="different-between-id3-and-cart">Different between ID3 and CART</h2>
<p>ID3, C4.5 and CART are all the algorithms in decision tree, since there's tiny different between ID3 and C4.5, here I just compare the two algorithms : ID3 and CART.</p>
<p><strong>The generating principle.</strong> In ID3 algorithm, we use <strong>information gain</strong> to determine the data set splitting, we choice the feature which has the largest value of information gain in each step, split the data set into plenty of units according to <span class="math inline">\(A_i = a_i\)</span>, which means the decision tree which is generated by ID3 may not be a binary tree, it's depends on the possible value of splitting features. However, in CART, we use <strong>Gini index</strong> as the splitting condition, we choice the feature which has the smallest gini index value in each step, split the data set according to whether <span class="math inline">\(A_i = a_i\)</span>, that means the CART is a <strong>binary tree</strong>. The similarity of two algorithms is on iterating, both the two methods need to calculate the information gain or gini index for each possible values for each features in each step.</p>
<p><strong>The pruning process.</strong> The basic idea of two pruning methods is similar, both of them use the idea of pre-pruning, which means the essence of pruning is judge whether it's worth to cut the brunch by comparing the loss function between before and after pruning. The pruning algorithm in CART is more advanced since the parameter <span class="math inline">\(\alpha\)</span> is not a fixed number, and the pruning is happened in the local area (actually, the pruning in ID3 can also be improved to cut in a local area cause each time we just cut one brunch in the tree, which means the difference of loss function in that local area has the same value with that in the entire tree). CART pruning algorithm uses a series <span class="math inline">\(\alpha\)</span> to find out the optimal tree instead of using a fixed value.</p>
<p><strong>All the codes in this atricle are collected on my github: <a href="https://github.com/dylanhouxinglin/lihang-statistical-learning-method/tree/master/lihang-code/DecisionTree" target="_blank" rel="noopener">Decision Tree</a></strong></p>
<h1 id="references">References</h1>
<ul>
<li><a href="http://www.dgt-factory.com/uploads/2018/07/0725/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.pdf" target="_blank" rel="noopener">统计学习方法</a></li>
<li><a href="https://blog.csdn.net/weixin_40604987/article/details/79296427" target="_blank" rel="noopener">Regression Tree 回归树</a></li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          
            <a href="/tags/Decision-Tree/" rel="tag"># Decision Tree</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/03/Regularization-in-Machine-Learnig/" rel="next" title="Regularization in Machine Learnig">
                <i class="fa fa-chevron-left"></i> Regularization in Machine Learnig
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/05/02/2019-5-2-近期问题总结/" rel="prev" title="2019.5.2 近期问题总结">
                2019.5.2 近期问题总结 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/head.png" alt="dylan_houxinglin">
            
              <p class="site-author-name" itemprop="name">dylan_houxinglin</p>
              <p class="site-description motion-element" itemprop="description">Monash University Master of Data Science, Recommender System / Machine Learning</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">23</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/dylanhouxinglin" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="xhou0008@student.monash.edu" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.zhihu.com/people/hou-xing-lin-72/activities" target="_blank" title="知乎">
                      
                        <i class="fa fa-fw fa-custom zhihu"></i>知乎</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#about-decision-tree"><span class="nav-number">1.</span> <span class="nav-text">About Decision Tree</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#feature-selection"><span class="nav-number">2.</span> <span class="nav-text">Feature Selection</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#entropy-and-conditional-entropy"><span class="nav-number">2.1.</span> <span class="nav-text">Entropy and Conditional Entropy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#information-gain"><span class="nav-number">2.2.</span> <span class="nav-text">Information Gain</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#concept-of-information-gain"><span class="nav-number">2.2.1.</span> <span class="nav-text">Concept of Information Gain</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#process-of-calculating-information-gain"><span class="nav-number">2.2.2.</span> <span class="nav-text">Process of Calculating Information Gain</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#information-gain-ratio"><span class="nav-number">2.2.3.</span> <span class="nav-text">Information Gain Ratio</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#decision-tree-generation"><span class="nav-number">3.</span> <span class="nav-text">Decision Tree Generation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#id3-algorithm"><span class="nav-number">3.1.</span> <span class="nav-text">ID3 Algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#steps"><span class="nav-number">3.1.1.</span> <span class="nav-text">Steps</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#code"><span class="nav-number">3.1.2.</span> <span class="nav-text">Code</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#c4.5-algorithm"><span class="nav-number">3.2.</span> <span class="nav-text">C4.5 Algorithm</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#decision-tree-pruning"><span class="nav-number">4.</span> <span class="nav-text">Decision Tree Pruning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#a-simple-pruning-approach"><span class="nav-number">4.1.</span> <span class="nav-text">A Simple Pruning approach</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#code-1"><span class="nav-number">4.2.</span> <span class="nav-text">Code</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#cart"><span class="nav-number">5.</span> <span class="nav-text">CART</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#regression-tree"><span class="nav-number">5.1.</span> <span class="nav-text">Regression Tree</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#principle-of-generating"><span class="nav-number">5.1.1.</span> <span class="nav-text">Principle of Generating</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#code-2"><span class="nav-number">5.1.2.</span> <span class="nav-text">Code</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#the-idea-of-classification"><span class="nav-number">5.1.3.</span> <span class="nav-text">The Idea of Classification</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#classification-tree"><span class="nav-number">5.2.</span> <span class="nav-text">Classification Tree</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#generating-principle"><span class="nav-number">5.2.1.</span> <span class="nav-text">Generating Principle</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#code-3"><span class="nav-number">5.2.2.</span> <span class="nav-text">Code</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cart-pruning"><span class="nav-number">5.3.</span> <span class="nav-text">CART Pruning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#pruning-algorithm"><span class="nav-number">5.3.1.</span> <span class="nav-text">Pruning Algorithm</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#code-4"><span class="nav-number">5.4.</span> <span class="nav-text">Code</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#different-between-id3-and-cart"><span class="nav-number">5.5.</span> <span class="nav-text">Different between ID3 and CART</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#references"><span class="nav-number">6.</span> <span class="nav-text">References</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="heart">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">dylan_houxinglin</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">25.6k</span>
  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




<sctipt async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">

        







        
      </sctipt></div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  















  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src>
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
